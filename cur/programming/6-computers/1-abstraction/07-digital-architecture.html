<!DOCTYPE html>
<html lang="en">
	<head>
		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<script type="text/javascript" src="/bjc-r/llab/loader.js"></script>
		<title>Unit 6 Lab 1: Computer Abstraction Hierarchy, Page 6</title>
	</head>
    <body>
        <div class="todo">
            <p><s>2.2.3C Code in a programming language is often translated into code in another (lower-level) language to be executed on a computer.</s> to 6.1.3<br />2.2.3I Hardware is built using multiple levels of abstractions, such as transistors, logic gates, chips, memory, motherboards, special purposes cards, and storage devices.</p>
		</div>
        
        <h2>The Digital Domain: Architecture</h2>
        <div class="learn"><strong>On this page</strong> we shift from software to hardware, starting with the <em>architecture,</em> which is essentially the hardware as it looks to the software, abstracting away the actual implementation of the architecture.</div>        
        <div class="todo">I want to do more to make this content flow nicely. Also, I'm not sure that it needs to be its own page. --MF, 11/8/17</div>
		<p>The software in a computer would be useless without the computer's <em>hardware:</em> the actual circuitry inside the box. Just as there are layers of abstraction for software, hardware designers also think in layers of abstraction.</p>
        <div class="comment">I find this information very helpful and might even consider putting it in a "takenote" block. --MF, 11/8/17</div>
        <p>Everyone talks about computers representing all data using only two values, 0 and 1. But that's not really how electronic circuits work. Computer designers can work <em>as if</em> circuits were either off (0) or on (1) because of the <strong>digital abstraction</strong>, the most important abstraction in hardware. Above that level of abstraction, there are four more detailed levels, called the <strong>digital domain</strong>. Below the digital abstraction, designers work in the <strong>analog domain</strong>, in which a wire in a circuit can have any voltage value, not just two values.    </p>

        <div class="sidenote">
<img src="/bjc-r/img/6-computers/charles-babbage.jpg" alt="Charles Babbage">
Charles Babbage<br />
<small>Public domain</small>
</div>
        <h3>The Stored Program Computer</h3>
        <p>There have been machines to carry out computations for thousands of years. (You'll see some examples in Lab 3.) But the modern, <em>programmable</em> computer had its roots in the work of Charles Babbage (1791-1871). Babbage was mainly a mathematician, but he contributed to fields as varied as astronomy and economics.</p>
        <p>Babbage lived about 150 years ago. Electricity as a source of energy was unknown. The steam engine came into widespread use around the time he was born. The most precise machinery of his time was clockworkâ€”gears.</p>
    <p>&nbsp;</p>

        <div class="endnote">
			<a href="#hint-difference" data-toggle="collapse">Babbage's first computer was the Difference Engine.</a>
			<div id="hint-difference" class="collapse">
<div class="sidenote">
<p><img src="/bjc-r/img/6-computers/babbage-difference-engine.jpg" alt="The Difference Engine at the London Science Museum">
The Difference Engine at the London Science Museum<br />
<small>Wikimedia user geni. Copyright 2008. License: GFDL, CC BY-SA.</small></p>

<p>
<img src="/bjc-r/img/6-computers/closeup-difference-eng.jpg" alt="A closeup showing the gears more clearly.">
A closeup showing the gears more clearly.
<br />
<small>Carsten Ullrich. Copyright 2005.  License: CC-BY-SA-2.5.</small></p></div>

            <p>He used gears to design a very complex machine that would
automatically compute and print tables of numbers, like the tables of log or
trig functions you may have in the back of a math textbook.  In Babbage's
time, such tables were computed by hand, by human mathematicians, and typeset
by hand for printing.  Both the computation and the copying into type were
error-prone, and accurate tables were needed for purposes ranging from
engineering to navigation.</p>

<p>Babbage built a first, small Difference Engine in 1822.  This first effort
proved that a Difference Engine was possible, but it didn't have the precision
(number of digits in each number) to be practical.  In 1823, the British
government funded Babbage to build a larger version.  Unfortunately,
metalsmiths in his day could not produce very precise gears in large
quantities; each one had to be handmade.  So he spent ten times his approved
budget by the time the government cancelled the project in 1842.</p>
<p>&nbsp;</p>


<p>In 1991, the London Science Museum completed a Difference Engine following
Babbage's original design, using gears made by modern processes but at the
level of precision that was available to Babbage.  This proved that in
principle Babbage could have completed a working machine, given enough time
and money.</p>

</div></div>

<H4>The Analytical Engine</H4>

<div class="sidenote">
<img src="/bjc-r/img/6-computers/punched-cards-analytical-engine.jpg" alt="punched cards used to program the Analytic Engine">
Punched cards used to program the Analytic Engine<br />
<small>Karoly Lorentey.  Copyright 2004.  License: CC-BY.</small></div>

<p>The Difference Engine could compute many different functions, by manually
setting the starting position of various gears.  But it had only one
algorithm, built into the hardware design.  In 1833, Babbage began work on a
much more ambitious project, the Analytical Engine.  It was based on the
general idea of the Difference Engine, but with a crucial difference:  It
could carry out instructions in a primitive programming language, prepared on
punched cards.</p>
<p>You've grown up surrounded by programmable computers, and software probably seems like an obvious idea to you. But it's not obvious, and before Babbage, algorithms were implemented directly in hardware.</p>

<p>The Analytical Engine, like modern computers, had an arithmetic processor
(called the "mill") and a separate memory (the "store") that would hold 1,000
numbers, each with up to 40 digits.  (In modern terms, this is comparable to
20,000 bytes, since a byte can hold two digits, with four bits needed for
each.)  By the way, the mill did arithmetic in decimal (with digits 0 to 9  equally spaced around each gear), so don't think that computers have to use just "ones and zeros."  The programming language included conditionals and looping, which
is all you need to represent any algorithm.  (It could loop because it could
move forward or backward through the punched cards containing the program.)</p>

<p>Alas, Babbage could build only a small part of the Analytical Engine, which
would have required even more metalworking than the Difference Engine.  His
notes about the design weren't complete, and so nobody has built a working
model, although there are simulations available on the Web.</p>

<div class="takeItFurther">
<ol type="A">
<li>Learning enough about the Analytical Engine to be able to write even a simple program for it is quite a large undertaking.  Don't even think about it until after the AP exam!  But there are extensive online resources available here:<br />
<a href="http://www.fourmilab.ch/babbage/contents.html">Table of Contents of web site</a>
<br />
<a href="http://www.fourmilab.ch/babbage/emulator.html">Online simulator</a></li></ol></div>

<p>So, 150 years ago, Babbage created plans for what is essentially a
modern computer, although he didn't have electronics available; his underlying
hardware was entirely mechanical.  Users would turn a crank to run the
machine; one or more turns of the crank would carry out one instruction in the
program.  In the early days of electronic computers, Babbage's work was not
widely known, and people ended up reinventing many of his ideas.</p>

        <div class="endnote">
			<a href="#hint-ada" data-toggle="collapse">Ada, Countess Lovelace invents symbolic computing.</a>
			<div id="hint-ada" class="collapse">

<div class="sidenote">
<img src="/bjc-r/img/6-computers/ada-lovelace.jpg" alt="Ada, Countess Lovelace">
Ada, Countess Lovelace<br />
<small>By Alfred Edward Chalon, <a rel="nofollow"
class="external text"
href="http://www.scienceandsociety.co.uk/results.asp?image=10312035">Science
&amp; Society Picture Library</a>, Public Domain, via Wikimedia.</small></div>

<p>Although his design was very versatile, Babbage himself was still mainly
interested in printing tables of numbers.  It was his collaborator Augusta Ada
King-Noel, Countess of Lovelace, who first recognized that the numbers in
Babbage's computer could be used not only as quantities but also as
representing musical notes, text characters, and so on.  </p>
<p>Much of what we know
  today about Babbage's design comes from Ada Lovelace's extensive notes on its
  design.  Those notes included the first <em>published</em> program for the
  Analytical Engine, and so she is widely considered "the first programmer,"
  although it's almost certain that Babbage himself wrote several
  example programs while designing the machine.</p>
<p>But, whether or not she was
truly the first programmer, historians all agree that she did something much
more important: she invented the idea of <em>symbolic</em> computation, as
opposed to numeric computation.  This insight paved the way for all the ways
that computers are used today, from movies on demand to voice-interactive
programs such as Siri and Alexa.</p>
</div></div>

<div class="takeNote">
Here are two key ideas to take away from this page:
<ol>
<li>What makes a computer usable for more than one purpose is the abstraction of <em>software,</em> or, in other words, a program stored in the computer's memory.<br />
<small>(Babbage's design was not quite the modern form of a stored program architecture, because the program memory was separate from the data memory.  That meant that the computer couldn't produce a program for itself.)</small></li>
<li>We didn't get <em>usable</em> computers until there was an underlying technology (the transistor, as we'll see shortly) small enough, inexpensive enough, and fast enough to support the program abstraction.</li></ol></div>
	
        <h3>What's an Architecture?</h3>
        <p>The processor in the computer you are using understands only one language, its own <em>machine language</em>, not Java, C, Snap<em>!</em>, Python, or anything else. To be run by your machine, programs written in those other programming languages must first be translated into machine language. The designers of a programming language must somehow include that translation. Languages like Snap<em>!</em> that run in a browser let the browser handle part of that translation. Other languages come in different versions depending on the machine and operating system.</p>
		<div class="vocabFullWidth"><strong>: Machine Language</strong>
<div class="todo"><h1>Crash content</h1>
		
            2.2.3C Code in a programming language is often translated into code in another (lower-level) language to be executed on a computer.<br />
            2.2.3I Hardware is built using multiple levels of abstractions, such as transistors, logic gates, chips, memory, motherboards, special purposes cards, and storage devices.
		</div>
        
        <h2>The Digital Domain: Architecture</h2>
        <div class="learn"><strong>On this page</strong>, we shift from software to hardware, starting with the <em>architecture,</em> the instructions that software can send that the hardware will understand.</div>        
        <div class="todo">
            <strong>From Michael:</strong><br />
            Also for any assembly stuff it'd be good to get input from Dan. I'd personally like to see MIPS assembly as an example over X86 since I think it's a bit more readable. ARM would be another reasonable choice.
            <ul><li>Brian, I'll leave this one to you... --MF, 11/17/17</li></ul>
            As far as including assembly: I think that gets to heart of the EK (LO?) about readability. 
        </div>
		<div class="todo">The whole idea of software depends on the crucial invention in the architecture of computing hardware, namely, the stored program architecture, which allows the machine to carry out any algorithmâ€”not just the particular one that's built in. it makes the hardware universal so the specification of what to do becomes the software which is much easier to changes.</div>
        <div class="todo">Need to integrate stuff from <a href="/bjc-r/cur/programming/6-computers/1-abstraction/07bh-digital-architecture.html?topic=nyc_bjc%2F6-how-computers-work.topic&course=bjc4nyc.html&novideo&noassignment" target="_blank">07bh-digital-architecture.html</a></div>
		
        
        <div class="todo">From 10/6/17 BH email: Compare Pentium, ARM, Arduino.  The major pieces of an architecture are
* Instruction set
* Memory width
* Multicore-ness
* Cache geometry
* RISC/CISC, pipelining, vector-ness, register set(s)...
We don't have to go into all of those, but at least the first three.  And, yes,
to talk about width you have to raise the question of bits, but at this point
you don't even have to call them that, just say that the memory bus has a bunch
of wires running in parallel through the computer, and the number of wires is
the width.
<p>In the world today, there are computers, there are telephones and tablets, and there are hobbiest kits (e.g., Arduino). The computers have all converged on the Intel architecture. With telephones, there are stilll two or three contenders (e.g., ARM). For hobiiest kits, there are the Arduino, the Raspberry Pi, and there are many Arduino compataible chips, because it's open architecture, and people keep inventing more. There are also embedded computers (like in cars and house-hold appliances). This is generally referred to as the "Internet of Things."</p>
</div>
		
		<p>The software in a computer would be useless without the computer's <em>hardware:</em> the actual circuitry inside the box. Just as there are layers of abstraction for software, hardware designers also think in layers of abstraction.</p>
 <div class="todo">The architecture is essentially the hardware as it looks to the software, abstracting away the actual implementation of the architecture.</div>
       <div class="todo">This is important, but I want to move it to The Analog Domain: Transistors
 --MF, 11/17/17
		<p>Everyone talks about computers representing all data using only two values, 0 and 1. But that's not really how electronic circuits work. Computer designers can work <em>as if</em> circuits were either off (0) or on (1) because of the <strong>digital abstraction</strong>, the most important abstraction in hardware. Above that level of abstraction, there are four more detailed levels, called the <strong>digital domain</strong>. Below the digital abstraction, designers work in the <strong>analog domain</strong>, in which a wire in a circuit can have any voltage value, not just two values.    </p>
        </div>
		
        <h3>What's an Architecture?</h3>
		<div class="vocab"><strong>: Machine Language</strong>
			<div class="todo">Pulled to the side because it's now just a reminder. --MF, 11/17/17</div>
</div>
			<p>Machine language is the lowest-level programming language, directly understood by the computer hardware.</p>
        </div>
<div class="todo"><h1>Crash content??</h1>
        <div class="sidenote">Processor has not mean defined yet. --MF, 11/17/17</div>
        <p>The processor in the computer you are using understands only one language, its own <em>machine language</em>. Not Java, C, Snap<em>!</em>, Python, or anything else. To be run by your machine, programs written in those other programming languages must first be translated into machine language. The designers of a programming language must somehow include that translation. Languages like Snap<em>!</em> that run in a browser let the browser handle part of that translation. Other languages come in different versions depending on the machine and operating system.</p>
</div>
        <p><em>Architecture</em> is an abstraction that specifies the machine language, instructions that the hardware will understand. It also tells how the processor connects to the memory. It doesn't include actual circuitry; that comes at a lower level of abstraction. </p>
        <p>One important part of an architecture is the number of wires that connect the processor and  memory. This is called the <em>width</em> of the architecture, measured in <em>bits.</em> The width is the number of data bits that the computer  processes in one instruction.</p>
        <div class="endnote">
			<a href="#hint-architecture" data-toggle="collapse">Learn about the PC/Mac architecture.</a>
			<div id="hint-architecture" class="collapse">
			  <div class="comment" style="color:grey">This has a lot of numbers in it which make it harder to read, but more importantly it's so abstract and doesn't really talk about anything familiar, which given the hint title "PC/Mac" I was expecting. Needs some work. --MF, 11/8/17</div>
			  <p>Most computer processors (the part that carries out instructions) in desktop or laptop computers use an architecture called "x86" that was designed at Intel, a chip manufacturer. The first processor using that architecture was called the 8086, released in 1978. (The reason for the name x86 is that the first few improved versions were called 80286, 80486, and so on.) The original 8086 was a 16-bit architecture; since then 32-bit (since 1985) and 64-bit (since 2003) versions have been developed. Even with all the refinements of the architecture,  the new x86 processors are almost always <em>backward compatible,</em> meaning that  today's versions  will still run  programs that were written for the original 8086.</p>
			  <p>Why did the x86 architecture come to rule the world? The short answer is that IBM used it in their original PC, and all the later PC manufacturers followed their lead because they could run IBM-compatible software unmodified. But why did IBM choose the x86? There were arguably better competing architectures available, such as the Motorola 68000 and IBM's own 801. The PC designers argued about which to use, but in the end, what made the difference was IBM's long history of working with Intel.</p>
			  <p>The Apple Macintosh originally used the Motorola 68000 architecture, and in 1994 Apple designed its own PowerPC architecture in a joint project with IBM and Motorola, but in 2006 they, too, switched to the x86, because Intel keeps producing newer, faster versions of the x86 more often than other companies could keep up.			</p>
			</div>
<div class="todo"><h1>Crash content</h1>
			  <div class="todo">This has a lot of numbers in it which make it harder to read, but more importantly it's so abstract and doesn't really talk about anything familiar, which given the hint title "PC/Mac" I was expecting. Needs some work next time around. --MF, 11/8/17</div>
              Most computer processors (the part that carries out instructions) in desktop or laptop computers use an architecture called "x86" that was designed at Intel, a chip manufacturer. The first processor using that architecture was called the 8086, released in 1978. The original 8086 was a 16-bit architecture; since then 32-bit (since 1985) and 64-bit (since 2003) versions have been developed. Even with all the refinements of the architecture,  the new x86 processors are almost always <em>backward compatible,</em> meaning that  today's versions  will still run  programs that were written for the original 8086. </div>
</div>
        </div>
<div class="todo"><h1>Crash content</h1>
        <div class="endnote">Learn about cell phone architecture.</div>
        <div class="endnote">Learn about hobbiest architectures.</div>
</div>
        <div class="endnote">
			<a href="#hint-architecture-phone" data-toggle="collapse">Learn about smartphone architecture.</a>
			<div id="hint-architecture-phone" class="collapse">
              <p>Everything about smartphone architecture is determined by the tiny size of the space inside the case.  The height and width of the phone are constrained by the size of people's front pockets. <em>(Don't keep your phone in your back pants pocket.  That's really bad both for the phone and for your back.)</em>            
            
            The front-to-back depth of a phone could be much bigger than it is, but for some reason phone manufacturers compete on the thinness of their phones, which gives designers even less room inside.</p>
              <p>As a result, many components that would be separate from the processor chip in a computer are instead part of the same chip in a phone. These components may include some or all of a cellular modem, a WiFi modem, a graphics processor (another processor that specializes in parallel arithmetic on lists of numbers), memory, a GPS receiver to find your phone's physical location, circuitry to manage the power depletion and recharging of the battery, and more. These days, the chip is likely to include two, four, or even eight copies of the actual CPU, to make multicore systems. This collection of components is called a <em>system on a chip,</em> or SoC.</p>
              <p>Intel made an x86-based (that is, the same architecture used in PCs) low-power SoC called the Atom, which was used in a few Motorola phones and some others made by companies you've never heard of. It was made to support Android, Linux, and Windows phones.</p>
              <p>But the vast majority of phones use the ARM architecture, which (unlike the x86) was designed from the beginning to be a low-power architecture. The acronym stands for Advanced RISC Machine. It's available in 32-bit and 64-bit configurations.</p>
        <div class="endnote">
			<a href="#hint-risc" data-toggle="collapse">What's a RISC?</a>
			<div id="hint-risc" class="collapse">
              <p>The name stands for Reduced Instruction Set Computer, as opposed to the CISC (Complex Instruction Set Computer) architectures, including the x86. The <em>instruction set</em> of an architecture is, as you'd guess from the name, the set of instructions that the processor understands. A RISC has fewer instructions than a CISC, but it's simpler in other ways also. For example, a CISC typically has more <em>addressing modes</em> in its instructions. In the x86 architecture, the <code>add</code> instruction can add two processor registers, or a register and a value from the computer's memory, or a constant value built into the instruction itself. A RISC architecture's <code>add</code> instruction just knows how to add two registers (perhaps putting the result into a third register), and there are separate <code>load</code> and <code>store</code> instructions that copy values from memory to register or the other way around. Also, in a RISC architecture, all instructions are the same length (say, 32 bits) whereas in a CISC architecture, instruction lengths may vary. These differences matter because a RISC can be loading the next instruction before it's finished with the previous instruction, and a RISC never has more than one memory data reference per instruction.</p>
              <p>So why don't they use a RISC architecture in PCs? At one time Apple used a RISC processor called the PowerPC in its Macintosh computers, but the vast majority of computers sold are PCs, not Macs, and as a result Intel spends vast sums of money on building faster and faster circuits implementing the x86 architecture. The moral is about the interaction between different levels of abstraction: A better architecture can be overcome by a better circuit design or better technology to cram components into an integrated circuit.</p>
			</div></div>
              <p>The company that designed the ARM, called ARM Holdings, doesn't actually build processors. They license either the architecture design or an actual circuit design to other companies that integrate ARM processors into SoCs. Major companies that build ARM-based processor chips include Apple, Broadcom, Qualcomm, and Samsung. Smartphone manufacturers buy chips from one of these companies.</p>
	</div></div>
        <div class="endnote">
			<a href="#hint-architecture-iot" data-toggle="collapse">Learn about embedded (Internet of Things) architecture.</a>
			<div id="hint-architecture-iot" class="collapse">
          <p>You can buy thermostats with computers in them, refrigerators with computers in them, fuzzy animal toys with computers in themâ€”more and more things, as time goes on.  Modern automobiles have <em>several</em> computers in them, largely for safety reasons; you wouldn't want the brakes to fail because the DVD player has a problem. The goal, as described by researchers in computing, is &quot;smart dust,&quot; meaning that lots of computers could be floating around a building unnoticed. What good is an unnoticed computer? This is a classic dual use technology. The beneficial use everyone talks about is emergency response to disasters; it would be a great help to the fire department to know, from the outside, which rooms of a building have people in them. But another use for this technology would be spying.          
          <p><img src="/bjc-r/img/6-computers/freescale_scmimx6d-sm.jpg" alt="Freescale SCM-i.MX6D chip, smaller than a dime">  <br />
          <small><small>NXP Freescale SCM-i.MX6D chip</small></small></p>
                  
          <p>For embedded computing,                                        		  
          the main design criteria are small size and low power consumption. The chip in the picture above is based on the ARM architecture, like most cell phones. That's actually a <em>big</em> embedded-systems chip; the Kinetis KL02 MCU (micro controller unit) fits in a 2 millimeter squareâ€”less than 1/10 inch.          That's still too big to float in the air like dust, but imagine it in a sticky container and thrown onto the wall.          
          <p><a href="https://www.engadget.com/2017/05/17/arm-targets-your-brain-with-new-implantable-chips/" target="new">ARM targets your brain with new implantable chips</a>                              
          <p>Intel made a button-sized x86-compatible chip in 2015, but announced in 2017 that it would be discontinued, leaving only ARM and PowerPC-based processors competing in this market.</p>
		  </div></div>
    <div class="endnote">
      <a href="#hint-architecture-hobby" data-toggle="collapse">Learn about hobbyist computer architecture.</a>
      <div id="hint-architecture-hobby" class="collapse">
<img class="imageLeft" src="/bjc-r/img/6-computers/i-void-warranties.jpg" width="183" alt="&quot;I void warranties&quot; t-shirt">
<p>In one sense, <em>any</em> architecture can be a hobbyist architecture. Even back in the days of million-dollar computers, there were software hobbyists who found ways to get into college computer labs, often by making themselves useful there.  Today, there are much more powerful computers that are cheap enough that hobbyists are willing to take them apart. But there are a few computer architectures <em>specifically</em> intended for use by hobbyists.</p>
<p>&nbsp;</p>
<img class="imageRight" 
src="/bjc-r/img/6-computers/arduino.jpg" alt="Arduino board">
<p>By far the most popular computer specifically for hobbyists is the Arduino. It's a circuit board, not just a processor. Around the edges of the board are connectors. On the short edge on the left in the picture are the power input, which can connect to a power supply plugged into the wall or to a battery pack for a mobile device such as a robot, and a USB connector used mainly to download programs from a desktop or laptop computer. On the long edges are connectors for single wires connected to remote sensors (for light, heat, being near a wall, touching another object, etc.) or actuators (stepping motors, lights, buzzers, etc.).
<p>One important aspect of the Arduino design is that it's <em>free</em> (&quot;free as in freedom&quot;). Anyone can make and even sell copies of the Arduino. This is good because it keeps the price down (the basic Arduino Uno board costs $22) and encourages innovation, but it also means that there can be incompatible Arduino-like boards. (The name &quot;Arduino&quot; is a trademark that can be used only by license from Arduino AG.)
<p>The processor in most Arduino models is an eight-bit RISC system with memory included in the chip, called the AVR, from a company called Atmel. It was designed by two (then) students in Norway, named Alf-Egil Bogen and Vegard Wollan. Although officially &quot;AVR&quot; doesn't stand for anything, it is widely believed to come from &quot;Alf and Vegard's RISC.&quot; There are various versions of the AVR processor, with different speeds, memory capacittes, and of course prices; there are various Arduino models using the different processors.
<p>Unlike most (&quot;von Neumann architecture&quot;) computers, the AVR (&quot;Harvard architecture&quot;) separates program memory from data memory. (It actually has <em>three</em> kinds of memory, one for the running program, one for short-term data, and one for long-term data.)
            Babbage's Analytical Engine was also designed with a program memory separate from its data memory.
<div class="endnote">
            <a href="#hint-Harvard" data-toggle="collapse">Why would you want more than one kind of memory?</a>
            <div id="hint-Harvard" class="collapse">
 <p>There are actually two different design issues at work in this architecture.  One is all the way down in the analog domain, having to do with the kind of physical circuitry used. There are <em>many</em> memory technologies, varying in cost, speed, and <em>volatility:</em> volatile memory loses the information stored in it when the device is powered off, while non-volatile memory retains the information. Here's how memory is used in the AVR chips: 
 <ul>
   <li><strong>EEPROM</strong> (512 Bytesâ€“4kBytes) is non-volatile, and is used for very long term data, like a file in a computer's disk, except that there is only a tiny amount available. Programs on the Arduino have to ask explicitly to use this memory, with an EEPROM library.
     <ul>
       <li>The name stands for Electrically Erasable Programmable Read-Only Memory, which sounds like a contradiction in terms. In the early days of transistor-based computers, there were two kinds of memory, volatile (Random Access Memory, or RAM) and nonvolatile (Read-Only Memory, or ROM). The values stored in an early ROM had to be built in by the manufacturer of the memory chip, so it was expensive to have a new one made. Then came Programmable Read-Only Memory (PROM), which was read-only once installed in a computer, but could be programmed, once only, using a machine that was only somewhat expensive. Then came EPROM, Erasable PROM, which could be erased in its entirety by shining a bright ultraviolet light on it, and then reprogrammed like a PROM. Finally there was Electrically Erasable PROM, which could be erased while installed in a computer, so essentially equivalent to RAM, except that the erasing is much slower than rewriting a word of RAM, so you use it only for values that aren't going to change often.</li>
       </ul>
   </li>
   <li><strong>SRAM</strong> (1kâ€“4kBytes): This memory can lose its value when the machine is turned off; in other words, it's volatile. It is used for temporary data, like the script variables in a Snap<em>!</em> script.
     <ul>
       <li>The name stands for Static Random Access Memory. The &quot;Random Access&quot; part differentiates it from the magnetic tape storage used on very old computers, in which it took a long time to get from one end of the tape to another, so it was only practical to write or read data in sequence. Today all computer memory is random access, and the name &quot;RAM&quot; really means &quot;writeable,&quot; as opposed to read-only. The &quot;Static&quot; part of the name means that, even though the memory requires power to retain its value, it <em>doesn't</em> require periodic refreshing as regular (&quot;Dynamic&quot;) computer main memory does. (&quot;Refreshing&quot; means that every so often, the computer has to read the value of each word of memory and rewrite the same value, or else it fades away. This is a good example of computer circuitry whose job is to maintain the <em>digital abstraction,</em> in which a value is zero or one, and there's no such thing as &quot;fading&quot; or &quot;in-between values.&quot;) Static RAM is faster but more expensive than dynamic RAM; that's why DRAM is used for the very large (several gigabytes) memories of desktop or laptop computers.</li>
       </ul>
   </li>
   <li><strong>Flash</strong> memory (16kâ€“256kBytes): This is the main memory used for programs and data. Flash memory is probably familiar to you because it's used for the USB sticks that function as portable external file storage. It's technically a kind of EEPROM, but with a different physical implementation that makes it much cheaper (so there can be more of it in the Arduino), but more complicated to use, requiring special control circuitry to maintain the digital abstraction.
     <ul>
       <li>&quot;More complicated&quot; means, for example, that changing a bit value from 1 to 0 is easy, but changing it from 0 to 1 is a much slower process that involves erasing a large block of memory to <em>all</em> 1 bits and then rewriting the values of the bits you didn't want to change.</li>
       </ul>
   </li>
   </ul>
 <p>So, that's why there are physically different kinds of memory in the AVR chips, but none of that completely explains the Harvard architecture, in which memory is divided into <em>program</em> and <em>data,</em> regardless of how long the data must survive. The main reason to have two different memory interface circuits is that it allows the processor to read a program instruction and a data value <em>at the same time.</em> This can in principle make the processor twice as fast, although that much speed gain isn't found in practice.</p>
 <p>To understand the benefit of simultaneous instruction and data reading, you have to understand that processors are often designed using an idea called <em>pipelining.</em> The standard metaphor is about doing your laundry, when you have more than one load. You wash the first load, while your dryer does nothing; then you <em>wash the second load while drying the first load, </em>and so on until the last load. Similarly, the processor in a computer includes circuitry to decode an instruction, and circuitry to do arithmetic. If the processor does one thing at a time, then at any moment either the instruction decoding circuitry or the arithmetic circuitry is doing nothing. But if you can read the next instruction at the same time as carrying out the previous one, all of the processor is kept busy.</p>
 <p>This was a long explanation, but it's still vastly oversimplified. For one thing, it's possible to use pipelining in a von Neumann architecture also. And for another, a <em>pure</em> Harvard architecture wouldn't allow a computer to load programs for itself to execute. So various compromises are used in practice.</p>
      </div></div>            
<p>Atmel has since introduced a line of ARM-compatible 32-bit processors, and Arduino has boards using that processor but compatible with the layout of the connectors on the edges.</p>
<div class="comment">Marlon J. Manrique via Wikimedia, CC-BY-SA 2.0.</div>
<img class="imageLeft" src="/bjc-r/img/6-computers/arduino-shields.jpg" width="400" alt="stack of Arduino shields">
<p>One thing that has contributed to the popularity of the Arduino with hobbyists is the availability of <em>shields,</em> which are auxiliary circuit boards that 

plug into the side edge connectors and have the same connectors on their top side. Shields add features to the system. Examples are motor control shields, Bluetooth shields for communicating with cell phones, RFID shields to read those product tags you find inside the packaging of many products, and so on. Both the Arduino company and others sell shields.
<p><div class="comment">Evan Amos, via Wikimedia, public domain</div><img  class="imageRight" src="/bjc-r/img/6-computers/raspberry-pi.jpg" width="400" alt="Raspberry Pi board">
<p>A completely different hobbyist architecture is the <em>Raspberry Pi.</em> It was designed to be used like a desktop or laptop computer, but with more access to its electronics. It uses an ARM-compatible processor, like most cell phones, but instead of running phone operating system software such as Android, it runs &quot;real&quot; computer operating systems. It ships with Linux, but people have run Windows on it.
<p>The main thing that makes it exciting is that it's inexpensive: different models range in price from $5 to $35. That price includes just the circuit board, as in the picture, without a keyboard, display, mouse, power adapter, or a case. The main expense in kit computers is the display, so the Pi is designed to plug into your TV. You can buy kits that include a minimal case, a keyboard, and other important add-ons for around $20. You can also buy fancy cases to make it look like any other computer, with a display, for hundreds of dollars.
<p>Because the Pi is intended for educational use, it comes with software, some of which is free for anyone, but some of which generally costs money for non-Pi computers. One important example is Mathematica, which costs over $200 for students (their cheapest price), but is included free on the Pi.
<p>Like the Arduino, the Pi supports add-on circuit boards with things like sensors and wireless communication modules.
</div></div>
    <div class="endnote">
            <a href="#hint-architecture-general" data-toggle="collapse">Learn more about computer architecture in general.</a>
            <div id="hint-architecture-general" class="collapse">
            	<div class="todo" style="color:grey">This needs some heavy edits. For example, perhaps the whole third paragraph, "One recent 64-bit x86..." could be cut. --MF, 11/8/17 </div>
		<h4>The memory hierarchy</h4>
<div class="todo"><h1>Crash content</h1>
            	<div class="todo">This needs some heavy edits. For example, perhaps the whole third paragraph, "One recent 64-bit x86..." could be cut. --MF, 11/8/17 </div>
				<h4>The memory hierarchy</h4>
</div>
                <p>For a given cost of circuit hardware, <strong>the bigger the memory, the slower it works.</strong> For this reason, computers don't just have one big chunk of memory. There will be a small number of <em>registers</em> inside the processor itself, usually between 8 and 16 of them. The &quot;size&quot; (number of bits) of a data register is  equal to the width of the architecture.</p>
                <p>The computer's main memory, these days, is measured in GB (gigabytes, or billions of bytes). A memory of that size can't be fast enough to keep up with a modern processor. Luckily, computer programs generally have <em>locality of reference,</em> which means that if the program has just made use of a particular memory location, it's probably going to use a nearby location next. So a complete program may be very big, but over the course of a second or so only a small part of it will be needed. Therefore,  modern computers are designed with one or more <em>cache</em> memoriesâ€”much smaller  and therefore fasterâ€”between the processor and the main memory. The processor makes sure that the most recently used memory is copied into the cache. </p>
                <p>One recent 64-bit x86 processor has a first level (L1) cache of 64KB (thousands of bytes) inside the processor chip, a larger but slower L2 cache of 256 KB, also inside the processor, and an L3 cache of up to 2 MB (megabytes, millions of bytes) outside the processor. Each level of cache has a copy of the most recently used parts of the next level outward: the L1 cache copies part of the L2 cache, which copies part of the L3 cache, which copies part of the main memory. Data in the L1 cache can be accessed by the processor about as fast as its internal registers, and each level outward is a little slower. Hardware in the processor handles all this complexity, so that programmers can write programs as if the processor were directly connected to the main memory.</p>
                
                <h4>Second sourcing</h4>
                <p>Intel licenses other chip manufacturers to build processors that use the same architecture as Intel's processors. Why do they do that? Wouldn't they make more money if people had to buy from Intel? The reason is that computer manufacturers, such as Dell, Apple, and Lenovo, won't build their systems around an architecture that is only available from one company. They're not worried that Intel will go out of business; the worry is that there may be a larger-than-expected demand for a particular processor, and Intel may not be able to fill orders on time. But if that processor is also available from other companies such as AMD and Cyrix, then a delay at Intel won't turn into a delay at Dell. Those other chip manufacturers may not use the same circuitry as the Intel version, as long as they behave the same at the architecture level.</p>
			</div>
        </div>
        
<div class="todo"><h1>Crash content</h1>
        <div class="todo">
        
        The most important part of the architecture is the machine language, the set of ultra-low-level instructions that the hardware understands.  This language is like a contract between the hardware and the software: The hardware promises to understand a set of instructions, and the software compiles programs from human-friendly language into those instructions.<br />

For example, take the Snap! instruction SET [C] TO (A + B).  In a lower-level language such as C or Java, the same idea would be written as<br />

<pre>c = a+b;</pre>

That simple command might be translated into six machine language instructions (slightly simplified here):<br />

        <pre>movq    _c, %rcx
movq    _b, %rdx
movq    _a, %rsi
movl    (%rsi), %edi
addl    (%rdx), %edi
movl    %edi, (%rcx)</pre>

This notation, called /assembly language/, is a line-by-line equivalent to the actual numeric instruction codes, but is slightly more readable.<br />

The first three instructions load the /addresses/ of the three variables into registers inside the processor.  The names with percent signs, such as %rcx, refer to specific processor registers.  MOVQ is the name of a machine language instruction. (It abbreviates "move quote," which says to move a constant value into a register.  Note that A is a variable, but /the address of/ A is a constant value -- the variable doesn't move around in the computer's memory.)  The next instruction, MOVL ("move long"), says to move a word from one place to another.  Putting a register name in parentheses, like "(%rsi)," means to use the memory location whose address is in the register.  In this case, since the third MOVQ put the address of A into register %rsi, the first MOVL says to move the variable A from memory into a processor register.  Then the ADDL instruction says to add the variable B into that same register.  Finally, the value in register %edi is moved into the memory location containing variable C.<br />

You wouldn't want to have to program in this language!  And you don't have to; modern architectures are designed for compilers, not for human machine language programmers.

</div>
		<div class="todo"><strong>Why not this?</strong> The point is for students to understand the different levels of abstraction of programming lanagues. Here they only learn about differences in syntax...</div>
		<div class="todo">
		<div class="forYouToDo" id="first">
			<ol>
				<li>
                    Look at these examples of the <em>same code</em> in different languages:
                    <div class="endnote">This code is a Fibonacci number calculator from Wikipedia's <a href="https://en.wikipedia.org/wiki/Low-level_programming_language" target="_blank">Low-level programming language</a> page.</div>
                    <ul>
                        <!--<li>Machine code:
                        <pre>8B542408 83FA0077 06B80000 0000C383
FA027706 B8010000 00C353BB 01000000
B9010000 008D0419 83FA0376 078BD989
C14AEBF1 5BC3</pre>
                        </li>-->
                        <li>Assembly, a notation for machine language:
                        <pre>
fib:
    mov edx, [esp+8]
    cmp edx, 0
    ja @f
    mov eax, 0
    ret
    
    @@:
    cmp edx, 2
    ja @f
    mov eax, 1
    ret
    
    @@:
    push ebx
    mov ebx, 1
    mov ecx, 1
    
    @@:
        lea eax, [ebx+ecx]
        cmp edx, 3
        jbe @f
        mov ebx, ecx
        mov ecx, eax
        dec edx
    jmp @b
    
    @@:
    pop ebx
    ret
</pre>
						</li>
                        <li>Higher-level C: 
                        <pre>
unsigned int fib(unsigned int n) {
    if (n &lt;= 0)
        return 0;
    else if (n &lt;= 2)
        return 1;
    else {
        unsigned int a,b,c;
        a = 1;
        b = 1;
        while (1) {
            c = a + b;
            if (n &lt;= 3) return c;
            a = b;
            b = c;
            n--;
        }
    }
}
</pre>
					</li>
                    </ul>
                    <div class="takeNote">Notice that even if you don't know C, you can still read some of the code. And what you can't read, you can probably imagine learning to read without much effort. The lower-level languages are <em>much</em> harder to learn.</div>
                </li>
                
			</ol>
		</div>
        </div>
        
        <div class="todo">
        	<div class="endnote">
                <h4><strong>The Analytical Engine</strong> (c. 1833 CE)</h4>
                <p>
                	The Difference Engine could compute various functions by manually setting the starting position of its gears, but it had only one algorithm, built into the hardware design.  In 1833, Babbage began a  more ambitious project, the Analytical Engine.  It was based on the general idea of the Difference Engine, but with a crucial difference:  It could carry out instructions in a primitive programming language, prepared on punched cards.
      <div class="sidenote">
                        <small>Image of punched cards used to program the Analytic Engine by Karoly Lorentey. Copyright 2004. License: CC-BY.</small>
                    </div>
                    <img src="/bjc-r/img/6-computers/punched-cards-analytical-engine.jpg" alt="punched cards used to program the Analytic Engine" title="punched cards used to program the Analytic Engine" />
                </p>
                <p>Like modern computers, the Analytical Engine had an arithmetic processor (called the "mill") and a separate memory (the "store") that would seem tiny by today's standards. The mill did its arithmetic in decimal, with digits 0 through 9  equally spaced around each gear.  The programming language included conditionals and looping,  all you need to represent any algorithm.  It could loop because it could move forward or backward through the punched cards.</p>
                <p>The Analytical Engine would have required even more metalworking than the Difference Engine so Babbage could build only a small part.  His design notes  weren't complete,  so nobody has built a working model, although you can find simulations  on the Web.</p>
              <p>So, almost 200 years ago, Babbage created plans for what is essentially a modern computer, though mechanical, not electronic.  Users would turn a crank to run the machine; one or more turns of the crank would carry out one instruction in the program.  In the early days of electronic computers, Babbage's work was not widely known, and people ended up reinventing many of his ideas.</p>
                <p>Babbage's design was also not quite the modern form of a stored program architecture, because the program memory was separate from the data memory.  That meant that the computer couldn't produce a program for itself. Modern computers can not only <em>run</em> programs, but can be programmed to <em>modify</em> and even <em>create</em> their own programs.</p>
            </div>
        </div>
		
		
</div>
        
	</body>
</html>

<!DOCTYPE html>
<html lang="en">
	<head>
		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<script type="text/javascript" src="/bjc-r/llab/loader.js"></script>
		<title>Unit 6 Lab 3: History and Impact of Computers, Page 1</title>
    </head>
    <body>
        <h2>A Brief History of Computers</h2>
        
        <div class="todo">Mary, can you make the timeline endnote hints close other open endnotes? --MF, 11/29/17</div>
		

        <div class="todo">
        	<h3>The First Computer</h3>
            <p>Why this?  Really the point is to broaden their sense of what a computer is,
            including the way computer architecture depends on the lower levels of
            abstraction.</p>
            <p>Why here?  Building up to the Analytical Engine on the next page.</p>
            
            <h3>From "The General Purpose Computer" page</h3>
            <p>Why this? This page is making the point that it's <em>programmability</em> that heralds the huge leap to modern computing, and that it was invented 200 years ago, well before it could actually be made practical.</p>
            <p>Why here? Following up on the first-page abstraction diagram, this is focusing in on the hardware/software barrier.</p>
        </div>
        
        <div class="learn"><strong>On this page</strong> you can dip into the very long history of devices to help people carry out computations.</div>

		<p>When was the first computer built? That depends on where you draw the line between computers and non-computers.</p>
		<p>This timeline shows a tiny selection of events in the history of computing. It starts 20,000 years ago! Click any date to see a bit about one example of a computational milestone from that time. Decide what <em>you</em> think makes sense to call the first real computer. (And, of course, look up more detail on any of these, if you want.) </p>
		<div class="comment">
            <p>Key:<br />
            <em>Italics</em>: particularly interesting.<br />
            <strong>Boldface</strong>: particularly important.<br />
            <strong><em style="color:red">Red boldface italics</em></strong>: The most important and interesting of all.</p>
        </div>

        <h3>Timeline</h3>
        <div class="todo"><h1>Paul/Brian, is this timeline final? I'd like to begin setting up the graphic (draft below; obviously too crowded and needs work). Selim, once the image is ready, can you make the HTML image map? --MF, 11/19/17</h1></div>
		<div class="todo">
            THIS IS JUST A DRAFT OF WHAT THIS MIGHT LOOK LIKE. (It doesn't work right yet.) Click these links and pretend you are clicking items on the timeline. We'll do that last after the image is 100% finalized.
            <ul>
            	<li><a href="#timeline-tally" data-toggle="collapse" class="collapsed" title="Hint">Tally Sticks (c. 18,000 BCE)</a><br />
                First known computing aid.</li>
            	<li><a href="#timeline-abacus" data-toggle="collapse" class="collapsed" title="Hint">Abacus (c. 2,000 BCE)</a><br />
Ancient calculator, still in use.</li>
            	<li><a href="#timeline-orrery" data-toggle="collapse" class="collapsed" title="Hint">Orrery (c. 100 BCE)</a><br />
Mechanical device that computes planets' orbits.</li>
                <li><a href="#timeline-difference-engine" data-toggle="collapse" class="collapsed" title="Hint"><strong>Difference Engine</strong> (1822)</a><br />
Charles Babbage: Mechanical single-purpose computer.</li>
                <li style="color:red"><a style="color:red" href="#timeline-analytical-engine" data-toggle="collapse" class="collapsed" title="Hint"><strong><em>Analytical Engine</em></strong> (1833)</a><br />
Babbage: First <em>stored-program</em> computer.</li>
                <li><a href="#timeline-first-programmer" data-toggle="collapse" class="collapsed" title="Hint"><strong>First Programmer</strong> (1842)</a><br />
Ada Lovelace: First to understand the potential of programs for <em>non-numeric</em> data.</li>
                <li><a href="#timeline-turing-wwii" data-toggle="collapse" class="collapsed" title="Hint"><em>Alan Turing wins World War II</em> (1940s)</a><br />
              Alan Turing leads a team that breaks the Enigma code using a very early electronic programmable computer.</li>
                <li><a href="#timeline-sketchpad" data-toggle="collapse" class="collapsed" title="Hint"><strong>Sketchpad</strong> (1963)</a><br />
              Ivan Sutherland: First object-oriented programming system, early interactive display graphics.</li>
                <li><a href="#timeline-nls" data-toggle="collapse" class="collapsed" title="Hint"><strong>First Mouse, Window System, etc.</strong> (1960s)</a><br />
Douglas Engelbart: An amazing number of firsts, including first hypertext, first collaborative editing, first video calling, much more.</li>
                <li><a href="#timeline-arpanet" data-toggle="collapse" class="collapsed" title="Hint">ARPANET (1969)</a><br />
The first version of what became the Internet.</li>
                <li><a href="#timeline-internet" data-toggle="collapse" class="collapsed" title="Hint">The Internet (1986)</a><br />
              First <em>network of networks</em>, based on TCP/IP.</li>
                <li><a href="#timeline-www" data-toggle="collapse" class="collapsed" title="Hint">The World Wide Web (1989-90)</a><br />
Tim Berners-Lee: First widely available hypertext (clickable links) system.</li>
                <li><a href="#timeline-smartphone" data-toggle="collapse" class="collapsed" title="Hint">First Smartphones (1990s)</a><br />
First portable devices combining cell phone with pocket computer.            </li>
                <li><a href="#timeline-chess" data-toggle="collapse" class="collapsed" title="Hint">Chess program beats world champion (2006)</a><br />
Deep Fritz program beats Vladimir Kramnik.</li>
                <li><a href="#timeline-siri" data-toggle="collapse" class="collapsed" title="Hint">Siri (2011)</a><br />Apple's personal assistant for the iPhone.</li>
                <li><a href="#timeline-pokemon-go" data-toggle="collapse" class="collapsed" title="Hint">Pokémon Go (2016)</a><br />
                  First widely used augmented reality game.       </li>
                <li><a href="#timeline-qx" data-toggle="collapse" class="collapsed" title="Hint">IBM Quantum Experience (2017)</a><br />
A 16-qubit quantum computer available for free use on the Internet.</li>
            </ul>
            <p>I think all the &quot;CE&quot;s are annoying; I propose we keep BCE but just leave the CE dates as just the number part. --bh  [PG agrees. Although using BCE and CE are less familiar (outside of academia) than BC and AD, they are more broadly inclusive, but there's no need to repeat CE (just as there'd be no need to repeat AD) and doing so just feels academic.]</p>
    </div>
    
    
    <div class = "sidenote"> <div class = "takeNote">Look at the timeline.  The year markers are equally spaced, but number of years between consecutive markers is not the same from marker to marker.  The number of years does <em>change</em> in a regular way from marker to marker.   Can you describe it precisely, so precisely that it can me modeled in <span class="snap">snap</span>?    </div></div>
        
        <img class="imageLeft noshadow" src="/bjc-r/img/6-computers/timeline.png" alt="timeline of computer history" title="timeline of computer history" />
        
        <div id="timeline-tally" class="timeline collapse">
        	<div class="endnote">
            	<h4><strong>Tally Sticks</strong> (c. 18,000 BCE)</h4>
            	<p>20,000 years ago people cut patterns of notches into animal bones. Some experts believe that these <em>tally sticks</em> were used to perform arithmetic computations.</p>
            </div>
        </div>
        
        <div id="timeline-abacus" class="timeline collapse">
            <div class="endnote">
            	<h4><strong>The Abacus</strong> (c. 2,000 BCE)</h4>
                <p>The <em>abacus</em> is a computing device invented about 4,000 years ago. People who are well-trained to use it can perform calculations remarkably quickly, including square roots and cube roots on multi-digit numbers. In some countries, the abacus is still widely used today.</p>
                <div class="sidenote">
                	<small>Image by <a href="https://fr.wikipedia.org/wiki/Utilisateur:HB" class="extiw" title="fr:Utilisateur:HB">HB</a> - <span class="int-own-work" lang="en">Own work</span>, Public Domain, from Wikimedia.</small>
                </div>
				<img src="/bjc-r/img/6-computers/abacus.jpg" alt="an abacus (a.k.a. a counting frame), a calculating tool used before written numerals" title="an abacus (a.k.a. a counting frame), a calculating tool used before written numerals" /> <br />
                The computation <em>algorithms</em> are executed by the user, not the device.
            </div>
        </div>  
        
        <div id="timeline-orrery" class="timeline collapse">
            <div class="endnote">
            	<h4><strong>The Orrery</strong> (c. 100 BCE)</h4>
                <p>
                    Since antiquity, many <em>special-purpose</em> computing devices have been invented. For example this <em>orrery</em> displayed the positions of the planets in the Solar System. The first known device of this kind dated from about 2100 years ago. People at that time generally believed that the sun and  other planets all revolved around the Earth, making the positions  very complicated to work out, so the mechanism was much harder to design and build than it would be today.
                    <div class="sidenote">
                        <small>No machine-readable image author provided. <a href="https://en.wikipedia.org/wiki/File:NAMA_Machine_d%27Anticyth%C3%A8re_1.jpg" target="_blank">Marsyas assumed</a> (based on copyright claims). GFDL, CC-BY-SA-3.0, or CC BY 2.5, via Wikimedia Commons.</small>
                    </div>
                    <img src="/bjc-r/img/6-computers/orrery1.jpg" alt="Gears from an ancient orrery" title="Gears from an ancient orrery" />
                </p>
                <p>
                    Here is a modern example of an orrery.
                    <div class="sidenote">
                        <small>Image by <a href="https://www.flickr.com/photos/kaptainkobold/127601212/sizes/m/" target="_blank">Kaptain Kobold</a>, licensed under the Creative Commons Attribution 2.0</small>
                    </div>
                    <img src="/bjc-r/img/6-computers/orrery2.jpg" alt="A modern orrery: mechanical model of planets" title="A modern orrery: mechanical model of planets" />
                </p>
            </div>
        </div>
        
        <div id="timeline-difference-engine" class="timeline collapse">
            <div class="endnote">            
                <h4><strong>The Difference Engine</strong> (c. 1822)</h4>
                <p>
                	In 1822, Charles Babbage designed a device he called a "Difference Engine." Made  of precisely milled metal gears, it would compute and print tables of numbers, like log or trig functions.  See more on <a href="/bjc-r/cur/programming/6-computers/1-abstraction/old/06-digital-architecture_broken.html">the digital architecture page</a>.</p></div>
</div>
        
        <div id="timeline-analytical-engine" class="timeline collapse">
            <div class="endnote">
                <h4><strong>The Analytical Engine</strong> (c. 1833)</h4>
                <p>
                	The Difference Engine could compute various functions by manually setting the starting position of its gears, but it had only one algorithm, built into the hardware design.  In 1833, Babbage began a  more ambitious project, the Analytical Engine.  It was based on the general idea of the Difference Engine, but with a crucial difference:  It could carry out instructions in a primitive programming language, prepared on punched cards.  See more on <a href="/bjc-r/cur/programming/6-computers/1-abstraction/old/06-digital-architecture_broken.html">the digital architecture page</a>.</p>
            </div>
        </div>
        
        <div id="timeline-first-programmer" class="timeline collapse">
            <div class="endnote">
            	<h4><strong>The First Programmer</strong> (c. 1842)</h4>
                <p>Babbage's design was very versatile, but his interest was still mainly in printing tables of numbers. It was his collaborator Ada Lovelace, who first recognized that the numbers in Babbage's computer could be used not only as quantities but also as representing musical notes, text characters, and so on.   See more on <a href="/bjc-r/cur/programming/6-computers/1-abstraction/old/06-digital-architecture_broken.html">the digital architecture page</a>.</p>        
            </div>
        </div>

        <div id="timeline-turing-wwii" class="timeline collapse">
          <div class="endnote">
            	<h4><strong>Alan Turing Wins World War II</strong> (1940s)</h4>
            	<p>Well, okay, we're exaggerating. Many people contributed to the Nazi defeat. But they weren't all soldiers; some were mathematicians. Turing, who went on to develop the beginning ideas of computer science and to prove the Halting Theorem (see <a href="/bjc-r/cur/programming/5-algorithms/4-unsolvable-undecidable/2-halting-problem.html">Unit 5</a>), was the leader of a British team of mathematicians who made a breakthrough in decrypting messages encrypted with the German <em>Enigma</em> machine.</p>
            	<p>Cryptanalysis of the Enigma enabled  western Allies in World War II to read substantial amounts of secret Morse-coded radio communications of the Axis powers. The western Supreme Allied Commander Dwight D. Eisenhower considered the military intelligence from this and other decrypted Axis radio and teleprinter transmissions "decisive" to the Allied victory.</p>
                <!-- <p>The Enigma machines were a family of portable cipher machines with rotor scramblers. The German plugboard-equipped Enigma became Nazi Germany's principal crypto-system. It was first broken by the Polish General Staff's Cipher Bureau in December 1932, with the aid of French-supplied intelligence material obtained from a German spy.</p> 
                <p>From this beginning, the British Government Code and Cypher School (GC&CS) at Bletchley Park built up an extensive cryptanalytic facility. Initially, the decryption was mainly of Luftwaffe and a few Army messages, as the Kriegsmarine (German navy) employed much more secure procedures for using Enigma. <strong> -->
                Alan Turing, a Cambridge University mathematician and logician, provided much of the original thinking and designed the device. Engineer Harold &quot;Doc&quot; Keen turned Turing's ideas into a working machine. 
                <!-- and the eventual breaking of naval Enigma. However, the German Navy introduced an Enigma version with a fourth rotor for its U-boats resulting in a prolonged period when these messages could not be decrypted. With the capture of relevant cipher keys and the use of much faster US Navy Bombes, regular, rapid reading of U-boat messages resumed. --></p>
                <!-- <p>The British decoder was an electromechanical device designed by Alan Turing soon after he arrived at Bletchley Park in September 1939. Harold "Doc" Keen of the British Tabulating Machine Company (BTM) <!-- in Letchworth (35 kilometres (22 mi) from Bletchley) was the engineer who turned Turing's ideas into a working machine. —under the codename CANTAB.[121] Turing's specification developed the ideas of the Poles' bomba kryptologiczna but was designed for the much more general crib-based decryption. </p> -->
                <p>[from Wikipedia CC-BY-SA.]</p>
            </div></div>

        <div id="timeline-sketchpad" class="timeline collapse">
          <div class="endnote">
           	<h4><strong>Sketchpad</strong> (1963)</h4>
<p>Ivan Sutherland's 1963 Ph.D. thesis project, a program to help in
 drawing blueprints from points, line
 segments and arcs of circles, pioneered both object
oriented programming and the ability to draw on a screen
  (using a light pen; the mouse hadn't been invented yet).  It was one of the first programs with an interactive graphical user interface, so people who weren't computer programmers could use it easily.</p>
<p>Search for &quot;Sketchpad&quot; on YouTube to see a demonstration of this software.</p>
          </div></div>

        <div id="timeline-nls" class="timeline collapse">
          <div class="endnote">
            	<h4><strong>&quot;The mother of all demos&quot; </strong>(1960s)</h4>
               <p>The first public demonstration in 1968 of a mouse, colleagues separated by miles  working on the same screen, and many other technologies we now take for granted was so astonishing to the audience that it became known as &quot;the mother of all demos.&quot; (Just search that name on Wikipedia or YouTube.)
<!-- It wouldn't astonish <em>you</em> so much, because everyone today is accustomed to using  many of the ideas that first appeared in this research. (You can watch it on YouTube if you're interested; just type &quot;mother of all demos&quot; in the YouTube search bar.) --></p> 
 <p>The inventor of this system, Douglas Engelbart, had a lifelong interest in using technology to augment human intelligence, in particular to support collaboration among people physically distant. He studied people doing intellectual work and noted that when they're not using a computer they don't sit rigidly in front of their desks; they wheel their chairs around the room as they grab books or just think. So he designed an office chair attachment of a lapboard containing a keyboard and a mouse—the first mouse. He also invented a way for people to collaborate on the same page at the same time, seeing each others' mouse cursors. Documents created in the system had hyperlinks to other documents, long before the Web was invented and used this idea. There were lots of smaller firsts, too, such as a picture-in-picture display of the other person's face camera, long before Skype. People remember Engelbart mostly because of the mouse, but he pioneered many features of the modern graphical user interface (GUI).</p></div></div>

        <div id="timeline-arpanet" class="timeline collapse">
          <div class="endnote">
            	<h4><strong>ARPANET</strong> (1969)</h4> 
                <p>The Advanced Research Projects Agency  
        (ARPA) of the US Defense Department was and is still the main funder of computer science research in the United States. In the late 1960s through the early 1980s they supported the development of a network connecting mainly universities with ARPA-funded projects, along with a few military bases. The initial network in 1969 consisted of four computers, three in California and one in Utah. At its peak, around 1981, there were about 200 computers on the net. With such a small number of computers, each of them knew the name and &quot;host number&quot; of all the others. Special gateway computers called IMPs (Interface Message Processors) were used to connect the host computers to the network, like a router today.</p>
                <p>Only organizations with ARPA research grants could be on the net. This meant almost all the network sites were universities, along with a small number of technology companies doing work for ARPA. Everyone knew everyone, and so the network was built around trust. People were encouraged to use other sites' resources; there was a yearly published directory of all the network computers, including, for most of them, information on how to log in as a guest user. It was a much friendlier spirit than today's Internet, with millions of computers and millions of users who don't know or trust each other. But the friendly spirit was possible only because access to the ARPANET was strongly restricted.</p>
                <p>The architects of the ARPANET knew that a system requiring every computer to know the address of every other computer wasn't going to work for a network accessible to everyone. Their plan was to build a <em>network of networks</em>—the Internet. The TCP/IP protocol stack was designed and tested on the ARPANET.</p>
          </div></div>
        
        <div id="timeline-internet" class="timeline collapse">
          <div class="endnote">
            	<h4><strong>Internet</strong> (1986)</h4> 
                <p>Gradually the ARPANET was divided into pieces. The first big pieces were MILNET for military bases and NSFnet, established in 1986 by the National Science Foundation, for research sites.  This split was the beginning of the Internet. Smaller regional networks were created.  Communication companies such as AT&amp;T created commercial networks that anyone could join.

Network access spread worldwide through satellite radio and through undersea cables. In 1995 the NSFnet was abolished, and everyone connected to the net through commercial Internet Service Providers.</p></div></div>        

        <div id="timeline-www" class="timeline collapse">
          <div class="endnote">
           	<h4><strong>World Wide Web</strong> (1989-90)</h4>
<p>In the early days of the net, the main application-level protocols were Telnet, which allowed a user to log in remotely to another computer, and FTP (File Transfer Protocol), for copying files from one computer to another.  FTP is great if you already know what you're looking for, and exactly where it is in the other computer's file system.</p>
<p>Several people had the idea of a system that would allow users to embed links to files into a conversation, so you could say &quot;I think <u>this document</u> might help you.&quot; In 1945, Vannevar Bush described a hypothetical device with the ability to embed links in files. In 1963, Ted Nelson made up the word &quot;hypertext&quot; as the name for this feature, but the first actual implementation was Douglas Engelbart's system NLS, developed starting in 1963 and demonstrated in 1968.</p>
<p>In 1989, physicist Tim Berners-Lee implemented a hypertext system and named it the World Wide Web. At first it was used only by physicists, to share data and ideas. But by then the Internet was running, so his timing was right, and the name was much more appealing than the technical-sounding &quot;hypertext&quot; (although, <a href="/bjc-r/cur/programming/4-internet/2-communication-protocols/3-open-protocols.html">as you know</a>, behind the scenes, the protocol that makes the Web work is called the &quot;HyperText Transfer Protocol&quot; or HTTP). The growth of the Web was very rapid, from about 500 servers in 1993 to over 10,000 in 1994. Today, many people talk as if  &quot;the Internet&quot; and &quot;the Web&quot; were the same thing, but <a href="/bjc-r/cur/programming/4-internet/1-reliable-communication/1-what-is-internet.html">you know better</a>.</p>
<p>Berners-Lee's vision was of a Web in which everyone would be both a creator and a consumer of information. But the Web quickly became a largely one-way communication, with a few commercial web sites getting most of the traffic. Today, most Web traffic goes to Google, Facebook, CNN, Amazon, and a few others. But some degree of democracy came back to the Web with the invention of the <em>blog </em>(short for &quot;web log&quot;) in which ordinary people can post their opinions and hope that other people will notice them.</p>
          </div></div>

    <div id="timeline-smartphone" class="timeline collapse">
      <div class="endnote">
           	<h4><strong>Smartphones</strong> (1990s)</h4>

<p><div class="sidenote"><img src="/bjc-r/img/6-computers/Kyocera6035.jpg" alt="Kyocera 6035 smartphone"><br />
<small><small>Kyocera 6035 (2001)<br />By KeithTyler, Public Domain</small></small></div></p>
<p>The first device that could be thought of as a smartphone was demonstrated in 1992 and available for sale in 1994.  But through the 1990s, people who wanted portable digital devices could get a cell phone without apps, and a &quot;personal digital assistant&quot; (PDA) that did run apps but couldn't make phone calls. Around 1999, a few companies developed devices with the phone and the PDA in one housing, sharing the screen but essentially two separate computers in one box. The Kyocera 6035, in the photo, was a telephone with the flap closed, but was a Palm Pilot PDA with the flap open.</p>
<p>It was in the late 2000s that the two functions of a smartphone were combined in a single processor, with the telephone dialer as just one application-level program.</p>
          </div></div>

    <div id="timeline-chess" class="timeline collapse">
      <div class="endnote">
       	<h4><strong>Chess program beats world champion</strong> (2006)</h4>
<p>Programmers have been trying to design and build chess programs since at least 1941 (Konrad Zuse). Over the following decade, several of the leading computer science researchers (including Alan Turing, in 1951) published ideas for chess algorithms, but the first actual running programs came in 1957. In 1978, a chess program won a game against a human chess master, David Levy, but Levy won the six-game match 4½–1½. (The half points mean that one game was a tie.)
<p>There are only finitely many possible chess positions, so <em>in principle</em> a program could work through all possible games and produce a complete <em>dictionary</em> of the best possible move for each player from each position. But there are about 10<sup>43</sup> board positions, and in 1950, Claude Shannon estimated that there are about 10<sup>120</sup> possible games, far beyond the capacity of even the latest, fastest computers. Chess programs, just like human players, can only work out all possible outcomes of the next few moves, and must then make informed guesses about which possible outcome is the best.
<p>A turning point in computer chess came in 1981, when the program Cray Blitz was the first to win a tournament, beat a human chess master, and earn a chess master rating for itself. The following year, the program Belle became the second computer program with a chess master rating. In 1988 two chess programs, HiTech and Deep Thought, beat human chess grandmasters.
<p>In 1997, Deep Blue, an IBM-built special-purpose computer  just to play chess, with 30 processors plus 480 special-purpose ICs to evaluate positions, beat world chess champion Garry Kasparov in a six-game match, 3½–2½. Its special hardware allowed Deep Blue to evaluate 200 million board positions per second.
<p>In 2006, world chess champion Vladimir Kramnik was defeated 4-2 in a six-game match by Deep Fritz, a chess program running on an ordinary computer. Although the score looks overwhelming, one of the games Deep Fritz won was almost a win for Kramnik, who failed to see a winning move for himself and instead set up the computer for a one-move checkmate. Without this blunder, the match would have been tied 3–3.
<p>Chess programs continue to improve. In 2009, the program Pocket Fritz 4, running on a cell phone, won a tournament and reached grandmaster rating. The program, in contrast to Deep Blue, could evaluate only 20,000 positions per second, so this win shows an improvement in strategy, not just an improvement in brute force computer speed.      </div></div>

    <div id="timeline-siri" class="timeline collapse">
      <div class="endnote">
       	<h4><strong>Siri</strong> (2011)</h4>
        <p>Siri is Apple's <em>personal assistant</em> software.  It was first released as a third-party app in the App Store in 2010; Apple then bought the company that made it, and included Siri as part of iOS in 2011.</p>
        <p>Siri was not the first program to understand speech. Dragon Dictate, a speech-to-text program was released in 1990. Research laboratory efforts started much earlier than that; in 1952, a program developed at Bell Labs was able to understand spoken digits. As time went on, the number of words understood by the programs increased. In 2002, Microsoft introduced a version of its Office programs (including Word) that would take spoken dictation.</p>
        <p>In 2006, the National Security Administration (NSA) started using software to recognize keywords in the telephone calls it spies on.</p>
        <p>The first cell phone app using speech recognition was Google's Voice Search in 2008, but it just entered the words it heard in a search bar without trying to understand them. What was new in Siri wasn't speech recognition, but its ability to understand the sentences spoken by its users as commands to do something: &quot;Call Fred,&quot; &quot;Make an appointment with Sarah for 3pm tomorrow,&quot; and so on.</p>
        <p>Reviews of Siri's performance in 2011 weren't all good. It had trouble understanding Southern US or Scottish accents. It had a lot of trouble with grammatically ambiguous sentences. Its knowledge of local landmarks was spotty. Nevertheless, it prompted a new surge of buyers of Apple telephones.</p>
        <p>More recently, Microsoft (Cortana), Amazon (Alexa), and Google (Assistant) have introduced competing speech-based personal assistant programs.</p>
      </div></div>

    <div id="timeline-pokemon-go" class="timeline collapse">
      <div class="endnote">
       	<h4><strong>Pokémon Go</strong> (2016)</h4>
            
<p><em>Augmented reality</em> is a technique in which the user sees the real world, but with additional pictures or text superimposed on it. Although it had been used earlier, the first major public exposure to augmented reality was in the game 
Pokémon Go. Players walk around while looking at their phone screens, which show what the camera is seeing, but with the occasional addition of a cartoon character for the player to catch. Every player looking at the same place sees the same character, because the game uses the phone's GPS to locate the player.
<p>The programming of the game was impressive, but even more impressive was the effort the developers put into placing the cartoon characters at locations around the world that are accessible, open to the public, and not offensive. (Niantic, the company that developed the game, had to remove some locations from their list because of complaints, including cemeteries and Holocaust museums.)
<p>The game was downloaded over 500 million times in 2016, and, unusually for a video game, was enthusiastically supported by many players' parents, because the game gets players out of the house, and getting exercise from walking around. Also, because players congregate at the locations of Pokémon, the game encouraged real-life friendships among players. On the other hand, there were safety concerns, partly because players would cross streets staring into their phones instead of watching for traffic, and partly because certain Pokémon were placed in front of fire stations, or in locations that encouraged players to cross railroad tracks.</p> </div></div>
    <div id="timeline-qx" class="timeline collapse">
      <div class="endnote">
       	<h4><strong>IBM Quantum Experience (QX)</strong> (2017)</h4>
        <p>IBM
        first put a five-qubit (quantum bit) quantum computer <a href="https://www.research.ibm.com/ibm-q/"  target="new">on the Internet</a> in 2016, but the following year they added a much more powerful 16-qubit computer. (The qubit is the equivalent for quantum computers of a bit in ordinary computers.) Anyone can use it, free of charge, although there is always a waiting list for appointments.        
        <p>The usual oversimplified description of quantum computing is that a qubit (pronounced &quot;Q bit&quot;) is &quot;both zero and one at the same time.&quot; It's closer to say that a qubit is either zero or one, but we don't know which until it is examined at the end of a computation, at which point it becomes an ordinary bit with a fixed value.         This means that a quantum computer with 16 qubits <em>isn't</em> quite as powerful as 2<sup>16</sup>      
        separate computers trying every possible combination of bit values in parallel. We know that <em>certain</em> exponential-time algorithms can be solved in polynomial time by quantum computers, and we know that certain others can't, but there is still a big middle ground of exponential-time algorithms for which we don't know how fast quantum computers can be.        
        <p>The IBM QX has been used for a wide variety of quantum computations, ranging from academic research to a multiplayer <a href="https://medium.com/@decodoku/quantum-battleships-the-first-multiplayer-game-for-a-quantum-computer-e4d600ccb3f3" target="new">quantum battleship</a> game.      
        </div></div>

    <br clear="all" />


	<div class="forYouToDo" id="first">
			<ol>
				<li>
                	Discussion:
                    <ul>
                        <li>Does a device have to be programmable to be a computer?</li>
                        <li>Does it have to operate by itself?</li>				   
                    </ul>
                </li>				   
			</ol>
		</div>
        
        <div class="takeNote">
            Here are two key ideas:
            <ul>
              <li><em>Software,</em> in the form of a program stored in the computer's memory, is, itself, a kind of abstraction. It is what makes a computer usable for more than one purpose.<br />
                    
              </li>
                <li>We didn't get <em>usable</em> computers until there was an underlying technology (the transistor) small enough, inexpensive enough, and fast enough to support the program abstraction.</li>
            </ul>
        </div>
            

	</body>
</html>


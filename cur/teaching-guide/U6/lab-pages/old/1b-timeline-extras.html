<!DOCTYPE html>
<html lang="en">
	<head>
		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<script type="text/javascript" src="/bjc-r/llab/loader.js"></script>
		<title>Unit 6 Lab 1: A Timeline of Computers, Page 2</title>
    <style type="text/css">
    most-important {
	color: #F00;
}
    </style>
	</head>
    <body>
        <h2>A Brief History of Computers</h2>
        <h3>Timeline Extras</h3>
        <div class="todo">
          <ul>
                <li><a href="#timeline-hollerith" data-toggle="collapse" class="collapsed" title="Hint"><em>First Punched Card Tabulator</em> (1890)</a><br />
            Herman Hollerith: Produced summaries of US Census data.</li>
                <li><a href="#timeline-analog" data-toggle="collapse" class="collapsed" title="Hint">Analog Computers (c. 1900)</a><br />
Special-purpose computers not based on zeros and ones.</li>
                <li><a href="#timeline-enigma" data-toggle="collapse" class="collapsed" title="Hint">Enigma (1930s)</a><br />
German mechanical computer for encoding secret messages.</li>
                <li><a href="#timeline-church" data-toggle="collapse" class="collapsed" title="Hint"><strong>Lambda Calculus, Undecidability</strong> (1936)</a><br />
Alonzo Church: Theoretical basis for <em>functional</em> programming.</li>
                <li><a href="#timeline-turing-machine" data-toggle="collapse" class="collapsed" title="Hint"><strong>Turing Machine, Halting Theorem</strong> (1936)</a><br />
Alan Turing: Theoretical basis for imperative programming.</li>
                <li><a href="#timeline-lisp" data-toggle="collapse" class="collapsed" title="Hint">LISP (1958)</a><br />
            John McCarthy: First functional programming language, still widely used.</li>
                <li><a href="#timeline-tcp-ip" data-toggle="collapse" class="collapsed" title="Hint">TCP/IP (1970s)</a><br />
            The design of the core protocols that run the Internet.</li>
                <li><a href="#timeline-alto" data-toggle="collapse" class="collapsed" title="Hint"><em>Xerox Alto</em> (1973)</a><br />
Alan Kay: Legendary prototype for personal interactive computing.</li>
                <li><a href="#timeline-first-pc" data-toggle="collapse" class="collapsed" title="Hint">First Personal Computer (1974)</a><br />
Altair 8800: First 8-bit personal computer.</li>
                <li><a href="#timeline-apple-ii" data-toggle="collapse" class="collapsed" title="Hint">Apple II (1977)</a><br />
Steve Wozniak: First mass-market personal computer.</li>
                <li><a href="#timeline-visicalc" data-toggle="collapse" class="collapsed" title="Hint">VisiCalc (1979)</a><br />
Dan Bricklin, Bob Frankston: First spreadsheet program.</li>
                <li><a href="#timeline-ibm-pc" data-toggle="collapse" class="collapsed" title="Hint">IBM PC (1981)</a><br />
First 16-bit computer marketed to businesses.</li>
                <li><a href="#timeline-mac" data-toggle="collapse" class="collapsed" title="Hint">Apple Macintosh (1984)</a><br />
First widely available computer using the ideas from the Alto: controlled mainly by mouse clicks rather than keyboard.    </li>
          </ul>
    </div>
        

        
        <div id="timeline-hollerith" class="timeline collapse">
            <div class="endnote">
            	<h4><strong>First Punched Card Tabulator</strong> (c. 1890)</h4>
                <p>Before the programmable digital computer, the first "big data" application was
                the US Census. Herman Hollerith developed a system of storing
                information encoded by punching holes in cardboard cards.  These cards were processed by non-programmable "tabulating equipment,"
                such as a card sorter, that could take a deck of cards and arrange them in
                numeric order based on a subset of the columns.  Hollerith's technology was bought by IBM and was the beginning of their
          involvement in computation.
                </p>
            </div></div>
        
        <div id="timeline-analog" class="timeline collapse">
          <div class="endnote">
           	<h4><strong>Analog Computers</strong> (c. 1900)</h4>
                          <div class="sidenoteBig">
         <img src="/bjc-r/img/6-computers/analog.png" alt="analog computer"><br /><small><small>analog computer<br />From http://chalkdustmagazine.com/</small></small></div>    

              <p>In analog computers,  a wire wasn't either fully on or fully off,
                but could carry any voltage (continuously variable) in the machine's operating
                range.  Because all voltages were legal, these machines suffered from
                electronic noise and so their precision was
                limited, typically to two or three decimal digits.  But for a certain class of
                problems—in particular, solving differential equations—these machines were a 
            natural fit.  (Babbage's Difference Engine solved a very similar class of problems, but with finite differences instead of continuous differentials.)</p>
            </div></div>
        
        <div id="timeline-enigma" class="timeline collapse">
          <div class="endnote">
           	<h4><strong>The Enigma</strong> (1930s)</h4>
            <div class="sidenoteBig">
         <img src="/bjc-r/img/6-computers/enigma.jpg" alt="Enigma machine model 1"><br /><small><small>Enigma model 1<br />By Alessandro Nassiri - Museo della Scienza e della Tecnologia "Leonardo da Vinci", CC BY-SA 4.0</small></small></div>    
            
              <p>The <strong>Enigma machines</strong> were a series of electro-mechanical rotor cipher machines developed and used in the early- to mid-20th century to protect  commercial, diplomatic and military communication. Enigma was invented  by the German engineer Arthur Scherbius at the end of World War I. Early models were used commercially from the early 1920s, and adopted  by military and government services of several countries, most notably Nazi Germany before and during World War II. Several different Enigma models were produced, but the German military models, having a plugboard, were the most complex. Japanese and Italian models were also in use.
              </p>
              <p>Though Enigma had some cryptographic weaknesses, in practice it was  German procedural flaws, operator mistakes, failure to systematically  introduce changes in encipherment procedures, and Allied capture of key  tables and hardware that, during the war, enabled Allied cryptologists  to succeed and "turned the tide" in the Allies' favor.</p>
              <p><small><small>from Wikipedia</small></small></p>
            </div></div>
        
        <div id="timeline-church" class="timeline collapse">
            <div class="endnote">
            	<h4><strong>Lambda Calculus</strong> (1936)</h4>
            <div class="sidenoteBig">
   <img src="/bjc-r/img/6-computers/church.png" width="194" height="251" alt="Alonzo Church">        <br /><small><small>Alonzo Church</small></small></div>    
             <p>Two people created a theoretical framework for computer science in the same year; Alonzo Church beat Alan Turing by just a few months.   But there is a crucial difference between their two approaches.  Turing's better-known idea, the <em>Turing machine,</em> represents a computation as a series of steps, with provision for conditionals and looping. Since the purpose of this theoretical machine is to prove theorems about computing, not to do actual computations, its data representation is limited to a &quot;tape,&quot; infinite in both directions, with cells that can contain one of a small number of symbols. The symbols are sometimes represented as 0 and 1, but this is slightly misleading because numbers are not represented in binary as on practical computers. Instead they use a conceptually simpler system called &quot;unary,&quot; although it's not a positional system like binary. The number five, for example, could be represented as 0111110, with five ones, delimited by a zero at both ends. So, representing the number 54321 would take 54,323 cells on the tape, all ones except for the two zeros at the end.</p>
             <p>The Turing machine is a theoretical description of <em>imperative</em> programming, with a program as a sequence of commands that operate by changing values in memory. This is why the AP CSP writers want you to learn that the fundamental building blocks of programs are <em>sequence, conditionals, and looping.</em> </p>
             <p>By contrast, Church's theoretical framework is based on <em>functions, </em>The basic operations in a program are procedure creation, like the grey rings in Snap<em>!</em>, and procedure calling, like the <code>run</code> block in Snap<em>!</em>. That turns out to be all you need for a complete programming language. All the rest—conditionals, looping, arithmetic, lists, Booleans, the works—can be created from those two operations.</p>
            </div></div>
        
        <div id="timeline-turing-machine" class="timeline collapse">
            <div class="endnote">
            	<h4><strong>The Turing Machine</strong> (1936)</h4>
                <p>
                </p></div></div>
        
        <div id="timeline-lisp" class="timeline collapse">
            <div class="endnote">
            	<h4><strong>LISP</strong> (1958)</h4>
                <p>
                </p></div></div>
        
        <div id="timeline-tcp-ip" class="timeline collapse">
            <div class="endnote">
            	<h4><strong>TCP/IP</strong> (1970s)</h4>
                <p>
                </p></div></div>
        
        <div id="timeline-alto" class="timeline collapse">
            <div class="endnote">
            	<h4><strong>The Xerox Alto</strong> (1973)</h4>
                <p>
                </p></div></div>
        
        <div id="timeline-first-pc" class="timeline collapse">
          <div class="endnote">
           	<h4><strong>The Altair 8800</strong> (1974)</h4>
              <div class="sidenoteBig">
            <img height="25%"  src="/bjc-r/img/6-computers/altair.jpg" alt="Altair 8800 computer"><br />  
             <small><small> Altair 8800 at the Computer History Museum<br />
              Todd Dailey, 2009, CC BY-SA 2.0</small></small></div>
            
              <p>The Altair was the first commercially successful desktop computer system in a box. The first microprocessor, the Intel 4004, came three years earlier, but during those three years anyone who wanted to use a microprocessor had to do a significant amount of engineering to combine the processor chip with memory and input/output circuitry. The Altair, with an eight-bit Intel 8080 processor chip, made it possible for hobbyists who were not electrical engineers to have a computer to program.</p>
              <p>Prior to this time, computers were essentially huge, expensive devices owned only by universities, some large businesses, and a small number of other well-funded institutions. 
              The Altair started a flood of inexpensive computers and computer kits. It was the beginning of the modern computer age.</p>
            </div></div>
        
        <div id="timeline-apple-ii" class="timeline collapse">
          <div class="endnote">
            	<h4><strong>The Apple II</strong> (1977)</h4>
              <div class="sidenoteBig">
         <img src="/bjc-r/img/6-computers/apple-ii.png" alt="Apple II with TV and cassette storage"><br /><small><small>Apple II, Panasonic RQ-2102 cassette, and TV<br />Photo credit: Carl Knoblock, Phil Pfeiffer </small></small></div>    
              <p>The Apple II, designed by Steve Wozniak using the eight-bit MOS Technology 6502 chip, was the first personal computer designed for the family market, rather than for intense hobbyists.  Because it included a keyboard, it used an ordinary TV set as its monitor, and it could use an audio cassette tape recorder as its external memory (instead of more expensive disk drives), people could buy one, unpack the box, plug it in, and it would be working.  It could display text and graphics in color, unlike earlier personal computer products. And it was inexpensive enough for parents to buy for their kids. </p>
            </div></div>
        
        <div id="timeline-visicalc" class="timeline collapse">
          <div class="endnote">
           	<h4><strong>VisiCalc</strong> (1979)</h4>
              <p>The first personal computer software intended specifically for business users was also the first interactive spreadsheet software on any computer.  Business planning has always been done using paper-and-pencil spreadsheets, but the process of creating them was lengthy and error-prone.  A program that could do the hard work and guarantee error-free results was eagerly welcomed by business.</p>
                <p>A spreadsheet program is actually a special-purpose programming language, because users enter mathematical formulas (that is, functions, with numbers and <em>other cells</em> as inputs) into the cells of the spreadsheet. which then computes numeric values for all the cells when a number is entered into a starter cell.</p>
                <p>VisiCalc sold hundreds of thousands of copies, and was responsible for much of the success of the Apple II, which businesses bought just as a platform for VisiCalc.</p>
            </div></div>
                        
        <div id="timeline-ibm-pc" class="timeline collapse">
          <div class="endnote">
            	<h4><strong>The IBM PC</strong> (1981)</h4>
              <div class="sidenoteBig">
    <img src="/bjc-r/img/6-computers/ibm-pc.jpg" alt="original IBM PC"><br />
<small><small>    Original IBM PC (Model 5150) with monochrome monitor<br />
By Ruben de Rijcke - Own work, CC BY-SA 3.0, via Wikimedia</small></small></div>
              <p>Businesses were slow to adopt personal computers.  Despite the popularity of VisiCalc, the eight-bit models seemed like toys
              and many businesses were reluctant to rely on them. This situation changed when IBM, the leader in large business computers, introduced their own personal computer, the 16-bit IBM PC. It was based on the Intel 8088, a variant of the 16-bit 8086 that used 16 bits inside the processor, but connected to memory and other components using an 8-bit bus, which made the other components slower (two bus operations needed per 16-bit word) but cheaper to build.</p>
                <p>Because of their position as a near-monopoly manufacturer of large business computers, a personal computer from IBM was taken seriously and made businesses confident in buying PCs. VisiCalc quickly released a version of their spreadsheet software for the PC, although it was exactly compatible with the 8-bit version and so it didn't take full advantage of the 16-bit processor and greatly expanded memory of the PC.</p>
                <p>Other companies started building 16-bit computers also, but at first they couldn't run software written for the IBM PC because IBM held the copyright on their operating system, PC-DOS. That OS was written for IBM by a young software company named Microsoft. They had a contract guaranteeing that Microsoft would not make PC-DOS available for non-IBM computers. This made it hard for competitors to get a start in the market.</p>
                <p>But then Microsoft betrayed IBM by introducing a new, different operating system, MS-DOS, for PC-compatible computers. MS-DOS was virtually identical to PC-DOS, but with a few small differences so that it wasn't technically a violation of their contract with IBM. But MS-DOS made it possible for other computers to run software written for the PC. The other manufacturers couldn't use the name &quot;PC,&quot; which was an IBM trademark, but users called them &quot;PC-compatible.&quot; These competing models were often less expensive than the PC, so they were able to get customers despite IBM's reputation for building rugged, reliable equipment. The IBM PC remained popular with businesses, especially ones that also had large IBM &quot;mainframe&quot; computers, but they no longer had a monopoly on 16-bit computing. Rather, it was Microsoft that held the monopoly, on the operating system they all used.</p>
            </div></div>
        
        <div id="timeline-mac" class="timeline collapse">
          <div class="endnote">
           	<h4><strong>The Apple Macintosh</strong> (1984)</h4>
                
              <img class="imageRight" src="/bjc-r/img/6-computers/mac.jpg" alt="original Apple Macintosh">
<p>When Steve Jobs visited Xerox PARC he was tremendously impressed by the user interface of the Alto.  He knew that a computer like that could be a huge seller, especially with people who were repelled by the PC-DOS user experience.  He hired away some of the PARC team to build a computer for Apple that would be similar to the Alto.</p>
<p> Apple's first try, the Lisa, was not a success; it was too expensive, and a little too much before its time.  But then came the Macintosh.  It was introduced to the world by an innovative advertisement on the 1984 Super Bowl broadcast, in which the PC was associated with the dictatorial society of George Orwell's novel <em>1984, </em>and the Mac freed the society. (You can find the ad on YouTube.) They also used the slogan &quot;the computer for the rest of us,&quot; which is still widely quoted. (People have, for example, described CS Principles as the computer science course for the rest of us.)</p>
<p>Since Microsoft quickly switched from PC-DOS to the Windows operating system that copied the Macintosh, you have grown up in a world in which the &quot;WIMP&quot; (windows, icons, menus, pointers) user interface is universally used on computers. So it may be hard for you to realize what a revolutionary change the Macintosh made. It was the first computer system you could just sit down and start using without reading the manual.</p>
            </div></div>
                        
        <br clear="all" />

		<div class="todo">
            <ul>
                <li><strong>Sketchpad</strong>:  Ivan Sutherland's 1963 Ph.D. thesis project, a program to help in
                drawing blueprints from points, line
                segments and arcs of circles.  It pioneered both the ability to draw on a screen
                (using a light pen; the mouse hasn't been invented yet) and object
            oriented programming.</li>
                
                <li><strong>Mouse, windows, and joint work on a single document</strong>:  <strong>(early 1960s) </strong>Douglas Engelbart had a lifelong interest in using technology to augment
                human intelligence,  in particular to support collaboration among people
                physically distant.  
                  <!-- The name comes from "oN-Line System" because he also had
                an "oFf-Line System" that's no longer important. -->
                  He studied people doing
                intellectual work and noted that when they're not using a computer they don't
                sit rigidly in front of their desks; they wheel their chairs around the room as
                they grab books or just think.  So he
                designed an office chair attachment of a lapboard containing a keyboard and a
                mouse—the first mouse.  
                  <!-- (Also a "chord keyboard" with
                five piano-key-like buttons so that you could position small amounts of text
                without having to move your hands back and forth between keyboard and mouse.) -->
                He also invented a way for  people to collaborate on the same page at the same time, seeing
                each others' mouse cursors.  Documents created in the system had hyperlinks to
                other documents, long before the Web used this idea.  There were lots of
                smaller firsts, too, such as a picture-in-picture display of the other person's
                face camera, long before Skype.
                
  People remember Engelbart mostly  because of the mouse, but he pioneered many features of the modern graphical user interface (GUI).</li>
                
                <li>* <strong>analog computer</strong>
                
                c. 1900 In analog computers,  a wire wasn't either fully on or fully off,
                but could carry any voltage (continuously variable) in the machine's operating
                range.  Because all voltages were legal, these machines suffered from
                electronic noise and so their precision was
                limited, typically to two or three decimal digits.  But for a certain class of
                problems—in particular, solving differential equations—these machines were a 
            natural fit.  They were programmed with plugboards, like ENIAC.</li>
                
                <li>* <strong>Hollerith's punch card tabulator for the Census</strong>
                
                Before the programmable digital computer, the first "big data" application was
                the US Census. Hollerith developed a system of storing
                information encoded by punching holes in cardboard cards.  These cards were processed by non-programmable "tabulating equipment,"
                such as a card sorter, that could take a deck of cards and arrange them in
                numeric order based on a subset of the columns.  Hollerith's technology was bought by IBM and was the beginning of their
          involvement in computation.</li>
                
                <li>* <strong>networked instant messaging</strong> (K. Harrenstein and B. Harvey)  :-)
                
                What we didn't invent was a centralized database of who's logged in on what
                computer; you had to know that the person you want to send a message to is at
                MIT, or is at Stanford.  (I'm not sure anyone else actually implemented this,
                although it was registered as an official Arpanet protocol, meaning just that
                we announced it in that RFC.)</li>
                
                <li><strong>the Enigma machine,
              Turing wins WWII</strong> [[There is text about this in the <a href="/bjc-r/cur/programming/5-algorithms/4-unsolvable-undecidable/2-halting-problem.html?topic=nyc_bjc%2F5-algorithms.topic&course=bjc4nyc.html&novideo&noassignment">unsolvable problem lab in U5</a>.]]</li>
                
                <li><strong>ENIAC programmers</strong> (include http://eniacprogrammers.org/see-the-film/ in TG),
                first widely used mechanical calculators (e.g., something from NCR or Monroe or IBM),</li>
                
                <li><strong>first personal computers</strong> (1974). Prior to this time, computers were essentially huge, expensive devices owned only by universities, some large businesses, and a small number of other well-funded institutions.
                
                <!-- [[Earlier than that, I think.  The Mac was (unforgettably) 1984, and the Apple II
                was several years before that, and the Altair was even earlier.
                
                I think it's worth saying that the idea of interactive computing is way older
                than that wave of small machines.  Back when computers were big and expensive,
                there were interactive terminals connected to timesharing systems, starting no
                later than CTSS in 1961.  The deep idea here is that pioneers develop ideas
                that are a little beyond the capabilities of the underlying technology, just as
                Logo was developed on a timesharing system but with the understanding that kids
                would eventually be able to run it on a personal computer.]] --></li>
                
                <li><strong>first smart phone</strong> early 1990s. Today's smartphones are vastly faster and more powerful and have vastly more memory than the huge multi-million dollar university computers of the  1970s. 
				</li>
                <li><strong>WWW</strong></li>
			</ul>
    </div>

		<div class="forYouToDo" id="first">
			<ol>
				<li>
                	Discussion:
                    <ul>
                        <li>Does a device have to be programmable to be a computer?</li>
                        <li>Does it have to operate by itself?</li>				   
                    </ul>
                </li>				   
			</ol>
		</div>
        
        <div class="takeNote">
            Here are two key ideas:
            <ul>
              <li><em>Software,</em> in the form of a program stored in the computer's memory, is, itself, a kind of abstraction. It is what makes a computer usable for more than one purpose.<br />
                    
              </li>
                <li>We didn't get <em>usable</em> computers until there was an underlying technology (the transistor) small enough, inexpensive enough, and fast enough to support the program abstraction.</li>
            </ul>
        </div>
            

	</body>
</html>

